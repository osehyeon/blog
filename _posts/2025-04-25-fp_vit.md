---
layout: single
title: "[논문 리뷰] FQ-ViT"
categories: "논문"
toc: true
typora-root-url: ./typora-root-url
---

[FQ-ViT](https://www.ijcai.org/proceedings/2022/164)는 Vision Transformer(ViT) 기반 모델에 대해 PTQ (Post-Training Quantization) 기반의 완전 정수 양자화를 적용하여, 정확도 손실 없이 경량화된 추론이 가능하도록 한 연구입니다.

FQ-ViT는 당시 MEGVII Technology의 연구원인 Yang Lin이 제1저자로 주도하였으며, 2022년 IJCAI에 발표되었습니다.

## 제안 기법 
- PTF (Power-of-Two Factor)
  - LayerNorm 입력의 채널 간 분포 차이를 보정하기 위한 기법입니다.
  - 각 채널에 2의 거듭제곱 계수를 곱해 스케일링합니다.
- LIS (Log-Int-Softmax)
  - 소프트맥스 함수의 출력이 대부분 0에 가까운 작은 값에 집중되어 있고  일부 값만 1에 근접하는 비균일한 분포를 가지고 있습니다.
  -  로그 기반 양자화 기법을 적용합니다.

### LayerNorm의 채널별 값 범위 및 채널별 최소/최대 값

<p align="center">
  <img src="../../images/2025-04-25-fp_vit/image-20250425010209284.png", style="width:80%;">
</p>

- 일반적으로 LayerNorm의 양자화는 전체 텐서에 동일한 스케일을 적용합니다.
- 채널 간 분포가 크게 다르면, 하나의 스케일로는 모든 채널의 특성을 제대로 반영하지 못해 심각한 양자화 오차가 발생합니다.
- PTF는 단일 스케일 구조를 유지하면서도, 각 채널에 대해 별도의 2의 거듭제곱 계수를 곱해 보정함으로써, 채널별 분포 차이를 반영하고 연산 효율도 유지합니다.

### 소프트맥스 이후 어텐션 스코어 분포 

<p align="center">
  <img src="../../images/2025-04-25-fp_vit/image-20250425010450030.png" style="width:70%;">
</p>

- Log2 양자화는 값이 작을수록 더 조밀하게 bin을 배치합니다.
- 따라서 Softmax처럼 작은 값이 대부분인 분포에서는, Log2 양자화가 균일 양자화보다 훨씬 정밀하게 값을 표현할 수 있습니다.

