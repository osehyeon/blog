---
layout: single
title: "[논문 리뷰] QPTQ"
categories: "논문"
toc: true
typora-root-url: ./typora-root-url
---

[GPTQ](https://arxiv.org/abs/2210.17323)는 Transformer 기반 대규모 언어 모델(LLM)에 대해 Post-Training Quantization (PTQ)을 적용하여, 매우 빠르고 효율적인 완전 정수 양자화를 가능하게 한 연구입니다.

GPTQ는 당시 IST Austria 소속의 Elias Frantar가 제1저자로 주도하였으며, 2023년 ICLR에 발표되었습니다.

