---
layout: single
title: "Autoregressive 모델과 KV Cache"
categories: "코딩"
toc: true
typora-root-url: ./typora-root-url
---

Autoregressive 모델 및 KV-Cache에 대해 다룬다. 

## Autoregressive Model

트랜스포머(Transformer)에서 오토리그레시브(autoregressive) 모델은 시퀀스를 앞에서부터 차례대로 생성하는 방식으로 동작합니다.  
이전까지의 토큰을 입력으로 받아 다음 토큰의 확률 분포를 예측합니다.
오토리그레시브 모델의 구조는 일반적으로 다음과 같습니다. 

<p align="center">
  <img src="../../images/2025-08-20-kv_cache/auto_model.png" style="width:50%;">
</p>

## Transformer Block 

트랜스포머 블록(Transformer Block)은 트랜스포머 모델을 구성하는 기본 단위 모듈입니다.
트랜스포머는 이러한 블록을 여러 층(layer) 쌓아 올려 만들어지며, 각 블록은 입력 시퀀스를 받아 Attention Block에서 Self-Attention을 통해 문맥을 이해하고, FFN Block에서 Feed-Forward Network를 통해 정보를 변환하는 과정을 담당합니다.
트랜스포머 블록의 구조는 다음과 같습니다. 

<p align="center">
  <img src="../../images/2025-08-20-kv_cache/transformer-5665862.png" style="width:50%;">
</p>

## Attention Block 

어텐션 블록의 구조는 아래와 같습니다.

<p align="center">
  <img src="../../images/2025-08-20-kv_cache/attention.png" style="width:50%;">
</p>
## KV Cache

위 Attention Block 구조를 보면 KV Cache 가 존재합니다.
KV Cache는 오토리그레시브 추론에서, 매번 새로운 토큰을 예측할 때 이전 토큰들에 대한 Key, Value를 다시 계산할 필요가 없도록 저장해 두는 메커니즘입니다.
이 메커니즘이 가능한 가장 큰 이유는 Self-Attention에서 과거 토큰의 Query는 더 이상 사용되지 않기 때문입니다.
필요한 것은 오직 과거의 Key, Value뿐이고, 이들은 고정되므로 캐시가 가능합니다.
