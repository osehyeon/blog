---
layout: single
title: "[논문 리뷰] I-ViT"
categories: "논문"
toc: true
typora-root-url: .
---

[I-ViT](https://arxiv.org/pdf/2207.01405)는 [I-BERT](https://arxiv.org/abs/2101.01321)에서 영감을 받아, Vision Transformer (ViT)에 정수 기반 연산을 도입한 연구입니다.

I-ViT는 당시 중국과학원대학 인공지능학부 석사과정 [Zhikai Li](https://scholar.google.com/citations?user=XwutB1AAAAAJ&hl=en)가 주도하였으며, 2023년 ICCV(International Conference on Computer Vision)에 발표되었습니다.


## 아키텍처

I-ViT는 트랜스포머 블록을 구성하는 LayerNorm, MatMul, Softmax, GELU 연산을 정수 연산으로 근사합니다.

<p align="center">
  <img src="/../images/2025-04-23-i_vit/image-20250424222311155.png" style="width:65%;">
</p>

1. LayerNorm의 제곱근 연산을 반복적인 선형 연산을 통해 근사합니다.

$$
   I_{i+1} = (I_i + \lfloor Var(I_x) / I_i \rfloor) / 2 = (I_i + \lfloor Var(x) / I_i \rfloor) \gg 1
$$

2. MatMul은 선형 연산임으로 정수 연산이 가능합니다.
3. Softmax를 구성하는 비선형 연산인 exp는 쉬프트 연산과 1차 선형 근사를 통해 선형 연산으로 구성합니다.

$$
2^x \approx \frac{x}{2} + 1, \quad \text{for } x \in (-1, 0]
$$

4. GELU는 Sigmoid를 사용하여 근사한다. Sigmoid는 exp로 구성되어 최종적으로 선형 연산으로 근사됩니다.

$$
   \text{GELU}(x) \approx x \cdot \sigma(1.702x), \quad \sigma(x) = \text{Sigmoid}(x)
$$



